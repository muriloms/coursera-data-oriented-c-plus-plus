1. Por que a varredura linear √© mais eficiente que a aleat√≥ria?
A resposta correta √©:

A varredura linear √© mais eficiente que a aleat√≥ria sempre que o conjunto de dados excede o tamanho do cache L1. Gra√ßas ao padr√£o previs√≠vel de acesso aos dados, novos dados podem ser pr√©-carregados (prefetched) enquanto a CPU trabalha nos dados atuais.

Explica√ß√£o:
Quando voc√™ acessa a mem√≥ria de forma sequencial (endere√ßo i, i+1, i+2...), a CPU detecta esse padr√£o simples e previs√≠vel. Um componente de hardware chamado prefetcher se antecipa e busca os pr√≥ximos blocos de mem√≥ria (as pr√≥ximas cache lines) da RAM (ou de um cache mais lento) para o cache L1 antes que a CPU precise deles. Isso efetivamente "esconde" a lat√™ncia da mem√≥ria, pois os dados j√° est√£o esperando no cache r√°pido quando s√£o solicitados. Esse benef√≠cio √© mais not√°vel quando os dados s√£o grandes demais para caberem no cache, for√ßando o sistema a buscar dados da mem√≥ria principal constantemente. üß†

2. Por que o acesso com passo (strided access) tamb√©m √© eficiente?
A resposta correta √©:

Os algoritmos de prefetch de cache s√£o geralmente capazes de detectar um padr√£o de acesso com passo e pr√©-carregar os dados de acordo.

Explica√ß√£o:
Os prefetchers de hardware em CPUs modernas s√£o mais sofisticados do que apenas detectar acessos sequenciais. Eles s√£o projetados para reconhecer outros padr√µes, incluindo o acesso com um passo constante (por exemplo, acessar os endere√ßos i, i+8, i+16, i+24...). Ao identificar esse padr√£o, o prefetcher come√ßa a buscar os blocos de mem√≥ria que cont√™m os pr√≥ximos elementos da sequ√™ncia, mesmo que n√£o sejam adjacentes, mantendo a efici√™ncia ao esconder a lat√™ncia da mem√≥ria.

3. Por que o acesso com passo de 64 bytes √© menos eficiente que o linear?
A resposta correta √©:

A CPU usa apenas um elemento por cache line. Sua opera√ß√£o √© muito r√°pida e n√£o deixa tempo suficiente para pr√©-carregar a pr√≥xima cache line.

Explica√ß√£o:
Uma cache line (a unidade de transfer√™ncia entre a RAM e o cache) tem tipicamente 64 bytes. Se o seu passo tamb√©m √© de 64 bytes, ocorre o seguinte:

O prefetcher carrega uma cache line de 64 bytes para o cache.

A CPU usa apenas um elemento (um double de 8 bytes) dessa linha. Os outros 56 bytes s√£o "desperdi√ßados" para aquele acesso.

Imediatamente, a CPU precisa de um dado que est√° na pr√≥xima cache line.

Nesse cen√°rio, a CPU consome os dados t√£o rapidamente (um por linha) que o prefetcher n√£o consegue buscar a pr√≥xima linha a tempo, for√ßando a CPU a esperar. Essa espera √© a lat√™ncia do cache L2 que n√£o p√¥de ser escondida.

4. Por que a performance cai ainda mais com um passo de 128 bytes?
Os efeitos que contribuem para essa desacelera√ß√£o s√£o:

O algoritmo de prefetch n√£o consegue ir al√©m dos limites da p√°gina. Com um passo maior, os limites de p√°gina s√£o atingidos com mais frequ√™ncia.

Com um passo maior, voc√™ consome p√°ginas de mem√≥ria a uma taxa mais alta. Isso leva a custosas falhas de cache TLB (TLB cache misses).

Explica√ß√£o:
A mem√≥ria n√£o √© apenas dividida em cache lines, mas tamb√©m em p√°ginas de mem√≥ria (geralmente de 4 kB). A tradu√ß√£o de endere√ßos virtuais para f√≠sicos para essas p√°ginas √© armazenada em um cache especial e muito r√°pido chamado TLB (Translation Lookaside Buffer).

Com passos muito grandes como 128 bytes, voc√™ cruza os limites dessas p√°ginas de mem√≥ria com muito mais frequ√™ncia. Cada vez que voc√™ acessa uma nova p√°gina, o sistema precisa de sua tradu√ß√£o de endere√ßo. Se essa tradu√ß√£o n√£o estiver no TLB (o que se torna comum, pois o TLB √© pequeno e voc√™ est√° "pulando" por muitas p√°ginas), ocorre um TLB miss. Resolver um TLB miss √© uma opera√ß√£o muito lenta que paralisa a CPU e degrada drasticamente a performance, explicando por que o tempo de acesso mais do que dobra.